{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vSvzvojN0bVs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc7137d-5040-4ff7-bd40-26fd3d454524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n",
            "Model's state_dict:\n",
            "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
            "conv1.bias \t torch.Size([6])\n",
            "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
            "conv2.bias \t torch.Size([16])\n",
            "fc1.weight \t torch.Size([120, 400])\n",
            "fc1.bias \t torch.Size([120])\n",
            "fc2.weight \t torch.Size([84, 120])\n",
            "fc2.bias \t torch.Size([84])\n",
            "fc3.weight \t torch.Size([10, 84])\n",
            "fc3.bias \t torch.Size([10])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#KPConv\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class KPConv(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, radius, sigma, bias = False, dimension = 3, inf = 1e6, epsilon = 1e-9):\n",
        "    \"\"\" \n",
        "    Args:  \n",
        "    in_channels : dimension of input features\n",
        "    out_channels : dimension of out features\n",
        "    kernel_size : number of kernel points\n",
        "    radius : radius used for kernel point init\n",
        "    sigma : influence radius of each kernel point\n",
        "    bias : use bias or not (default: False)\n",
        "    dimension : dimension of the point space\n",
        "    inf : value of infinity to generate the padding point\n",
        "    epsilon: epsilon for gaussian influence\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super(KPConv,self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.kernel_size = kernel_size\n",
        "    self.radius = radius\n",
        "    self.sigma = sigma\n",
        "\n",
        "    self.dimension = dimension\n",
        "    self.inf = inf\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "    #initialize weights\n",
        "\n",
        "    self.weights = torch.nn.Parameter(torch.zeros(self.kernel_size,self.in_channels,self.out_channels))\n",
        "\n",
        "    if bias:\n",
        "      self.bias = torch.nn.Parameter(torch.zeros(self.out_channels))\n",
        "    else:\n",
        "      self.register_parameter('bias',None)\n",
        "           \n",
        "    self.reset_parameters()\n",
        "\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      torch.nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5))\n",
        "\n",
        "      if self.bias is not None:\n",
        "        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in) \n",
        "        torch.nn.init.uniform(self.bias,-bound,bound)\n",
        "    \n",
        "    def initialize_kernel_points(self):\n",
        "      \n",
        "\n",
        "\n"
      ]
    }
  ]
}